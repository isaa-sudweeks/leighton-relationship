{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utah Air Quality Data Scraper\n",
    "\n",
    "This notebook scrapes historical air quality data for Utah from the EPA AQS API.\n",
    "\n",
    "**Parameters:**\n",
    "- Ozone (O3)\n",
    "- Nitric Oxide (NO)\n",
    "- Nitrogen Dioxide (NO2)\n",
    "- Solar Radiation (SR)\n",
    "- Temperature (Temp)\n",
    "\n",
    "**Output:**\n",
    "- Raw Data: `data/{site_name}_raw.csv`\n",
    "- Cleaned Data: `data/{site_name}_cleaned.csv` (Rows with missing values dropped)\n",
    "- Metadata: `data/{site_name}_metadata.json`\n",
    "\n",
    "**Note:** This script pulls **Hourly Data**, which may take a significant amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# Ensure data directory exists\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# Constants\n",
    "BASE_URL = \"https://aqs.epa.gov/data/api\"\n",
    "STATE_CODE = \"49\"  # Utah\n",
    "\n",
    "# Parameter Codes\n",
    "PARAMS = {\n",
    "    \"44201\": \"O3\",\n",
    "    \"42601\": \"NO\",\n",
    "    \"42602\": \"NO2\",\n",
    "    \"63301\": \"SR\",\n",
    "    \"62101\": \"Temp\"\n",
    "}\n",
    "PARAM_CODES_STR = \",\".join(PARAMS.keys())\n",
    "\n",
    "# Helper for rate limiting\n",
    "def wait_for_api():\n",
    "    time.sleep(1) # Be polite to the API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Authentication\n",
    "Enter your EPA AQS API credentials below. If you don't have them, run the signup cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREDENTIALS\n",
    "API_EMAIL = \"test@test.com\" # REPLACE WITH YOUR EMAIL\n",
    "API_KEY = \"test\"   # REPLACE WITH YOUR KEY\n",
    "\n",
    "def signup(email):\n",
    "    url = f\"{BASE_URL}/signup?email={email}\"\n",
    "    response = requests.get(url)\n",
    "    print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Site Discovery\n",
    "Get all sites in Utah."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sites(email, key, state_code):\n",
    "    # 1. Get Counties\n",
    "    print(\"Fetching counties...\")\n",
    "    counties_url = f\"{BASE_URL}/list/countiesByState?email={email}&key={key}&state={state_code}\"\n",
    "    resp = requests.get(counties_url)\n",
    "    if resp.status_code != 200:\n",
    "        print(\"Error fetching counties:\", resp.text)\n",
    "        return []\n",
    "    \n",
    "    counties = resp.json().get('Data', [])\n",
    "    all_sites = []\n",
    "    \n",
    "    # 2. Get Sites for each County\n",
    "    print(f\"Found {len(counties)} counties. Fetching sites...\")\n",
    "    for county in counties:\n",
    "        county_code = county['code']\n",
    "        # sitesByCounty\n",
    "        sites_url = f\"{BASE_URL}/list/sitesByCounty?email={email}&key={key}&state={state_code}&county={county_code}\"\n",
    "        s_resp = requests.get(sites_url)\n",
    "        wait_for_api()\n",
    "        \n",
    "        if s_resp.status_code == 200:\n",
    "            sites_data = s_resp.json().get('Data', [])\n",
    "            for site in sites_data:\n",
    "                # Enrich with county info for easier reference\n",
    "                site['county_code'] = county_code\n",
    "                site['county_name'] = county['value_represented']\n",
    "                all_sites.append(site)\n",
    "        else:\n",
    "            print(f\"Warning: Could not fetch sites for county {county_code}\")\n",
    "\n",
    "    # Deduplicate sites just in case (though list/sitesByCounty shouldn't overlap)\n",
    "    # A site is uniquely identified by state-county-site_number\n",
    "    unique_sites = {}\n",
    "    for s in all_sites:\n",
    "        uid = f\"{state_code}-{s['county_code']}-{s['code']}\"\n",
    "        unique_sites[uid] = s\n",
    "    \n",
    "    return list(unique_sites.values())\n",
    "\n",
    "# Check Monitors to pre-filter sites (Only keep sites that have ALL 5 params)\n",
    "def check_site_monitors(email, key, state, county, site):\n",
    "    # list/monitorsBySite\n",
    "    url = f\"{BASE_URL}/list/monitorsBySite?email={email}&key={key}&state={state}&county={county}&site={site}\"\n",
    "    resp = requests.get(url)\n",
    "    wait_for_api()\n",
    "    if resp.status_code != 200:\n",
    "        return False, []\n",
    "    \n",
    "    monitors = resp.json().get('Data', [])\n",
    "    found_params = set()\n",
    "    for m in monitors:\n",
    "        if m['parameter_code'] in PARAMS:\n",
    "            found_params.add(m['parameter_code'])\n",
    "            \n",
    "    # Function returns True if ALL 5 are present\n",
    "    # PARAMS.keys() are strings in the set\n",
    "    required = set(PARAMS.keys())\n",
    "    \n",
    "    # STRICT CHECK: Must have ALL 5\n",
    "    has_all = required.issubset(found_params)\n",
    "    return has_all, list(found_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Fetching\n",
    "Fetch hourly data for valid sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_hourly_data(email, key, state, county, site, bdate, edate):\n",
    "    # Fetch data for all params at once\n",
    "    # URL: sampleData/bySite\n",
    "    url = f\"{BASE_URL}/sampleData/bySite?email={email}&key={key}&param={PARAM_CODES_STR}&bdate={bdate}&edate={edate}&state={state}&county={county}&site={site}\"\n",
    "    resp = requests.get(url)\n",
    "    wait_for_api()\n",
    "    if resp.status_code != 200:\n",
    "        print(f\"    Error fetching {bdate}-{edate}: {resp.status_code}\")\n",
    "        return []\n",
    "    \n",
    "    return resp.json().get('Data', [])\n",
    "\n",
    "def sanitize_filename(name):\n",
    "    return re.sub(r'[^\\w\\-_]', '_', name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Main Execution Loop\n",
    "This loop will:\n",
    "1. Get all sites.\n",
    "2. Filter for sites that have ALL 5 parameters.\n",
    "3. Fetch hourly data year by year.\n",
    "4. Clean and Save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date Range to search\n",
    "START_YEAR = 1980\n",
    "END_YEAR = datetime.now().year\n",
    "\n",
    "# 1. Get List of Sites\n",
    "all_sites_list = get_sites(API_EMAIL, API_KEY, STATE_CODE)\n",
    "print(f\"Total Sites in Utah: {len(all_sites_list)}\")\n",
    "\n",
    "for site_info in all_sites_list:\n",
    "    site_id = site_info['code']\n",
    "    county_code = site_info['county_code']\n",
    "    site_name = site_info.get('value_represented', f\"Site_{site_id}\") # Use value_represented as name if available\n",
    "    # Fallback if value_represented is empty or just a number, try local site name\n",
    "    if not site_name or site_name.strip() == \"\":\n",
    "         site_name = site_info.get('local_site_name', f\"Site_{site_id}\")\n",
    "\n",
    "    safe_name = sanitize_filename(site_name)\n",
    "    print(f\"Processing Site: {site_name} (ID: {site_id}) in County {county_code}...\")\n",
    "\n",
    "    # 2. Check Monitors (Pre-filter)\n",
    "    has_all_params, found_params = check_site_monitors(API_EMAIL, API_KEY, STATE_CODE, county_code, site_id)\n",
    "    if not has_all_params:\n",
    "        print(f\"  Skipping. Missing parameters. Found: {[PARAMS.get(p, p) for p in found_params]}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"  Site has all parameters! Fetching data...\")\n",
    "    \n",
    "    all_records = []\n",
    "    \n",
    "    # 3. Fetch Data Year by Year\n",
    "    for year in range(START_YEAR, END_YEAR + 1):\n",
    "        bdate = f\"{year}0101\"\n",
    "        edate = f\"{year}1231\"\n",
    "        \n",
    "        # Skip future dates\n",
    "        if year == datetime.now().year:\n",
    "             today_str = datetime.now().strftime(\"%Y%m%d\")\n",
    "             if bdate > today_str: break\n",
    "             edate = min(edate, today_str)\n",
    "\n",
    "        print(f\"    Fetching {year}...\")\n",
    "        year_data = fetch_hourly_data(API_EMAIL, API_KEY, STATE_CODE, county_code, site_id, bdate, edate)\n",
    "        all_records.extend(year_data)\n",
    "    \n",
    "    if not all_records:\n",
    "        print(\"  No data records found.\")\n",
    "        continue\n",
    "\n",
    "    # 4. Processing\n",
    "    print(f\"  Processing {len(all_records)} records...\")\n",
    "    df = pd.DataFrame(all_records)\n",
    "\n",
    "    # Relevant columns: 'date_gmt', 'time_gmt', 'parameter_code', 'sample_measurement', 'units_of_measure'\n",
    "    # We need to construct a usage Timestamp\n",
    "    # Combine Date/Time\n",
    "    # Note: Using GMT or Local? API returns both. Let's use Local for standard analysis if preferred, or GMT.\n",
    "    # User didn't specify, but usually Local Standard Time (date_local, time_local) is best for air quality trends.\n",
    "    df['datetime'] = pd.to_datetime(df['date_local'] + ' ' + df['time_local'])\n",
    "    \n",
    "    # Map Parameter Codes to Names\n",
    "    df['param_name'] = df['parameter_code'].map(PARAMS)\n",
    "    \n",
    "    # Pivot\n",
    "    # Index: datetime\n",
    "    # Columns: param_name\n",
    "    # Values: sample_measurement\n",
    "    # Aggregation: mean (in case of duplicates, though sampleData shouldn't have them for same POC. \n",
    "    # If multiple POCs exist for same param, we average them? Or take the first? \n",
    "    # Averaging across POCs is safer for 'site value')\n",
    "    df_pivoted = df.pivot_table(index='datetime', columns='param_name', values='sample_measurement', aggfunc='mean')\n",
    "\n",
    "    # 5. Saving Raw\n",
    "    raw_file = f\"data/{safe_name}_raw.csv\"\n",
    "    df_pivoted.to_csv(raw_file)\n",
    "    print(f\"  Saved RAW: {raw_file}\")\n",
    "\n",
    "    # 6. Cleaning\n",
    "    # Strict Drop: Row must have O3, NO, NO2, SR, Temp\n",
    "    required_cols = ['O3', 'NO', 'NO2', 'SR', 'Temp']\n",
    "    # Improve robustness: Only drop if columns exist. But we already checked `check_site_monitors` so they should be there.\n",
    "    # However, if data fetch returned nothing for a specific param for that year, the column might be missing in pivot.\n",
    "    # Reindex to ensure all columns exist\n",
    "    df_pivoted = df_pivoted.reindex(columns=required_cols)\n",
    "    \n",
    "    df_cleaned = df_pivoted.dropna()\n",
    "    \n",
    "    cleaned_file = f\"data/{safe_name}_cleaned.csv\"\n",
    "    df_cleaned.to_csv(cleaned_file)\n",
    "    print(f\"  Saved CLEANED: {cleaned_file} ({len(df_cleaned)} rows)\")\n",
    "\n",
    "    # 7. Metadata\n",
    "    # Extract units\n",
    "    units = {}\n",
    "    for p_code, p_name in PARAMS.items():\n",
    "         # Find first record with this code to get unit\n",
    "         subset = df[df['parameter_code'] == p_code]\n",
    "         if not subset.empty:\n",
    "             units[p_name] = subset.iloc[0]['units_of_measure']\n",
    "         else:\n",
    "             units[p_name] = \"N/A\"\n",
    "\n",
    "    metadata = {\n",
    "        \"site_name\": site_name,\n",
    "        \"site_id\": site_id,\n",
    "        \"county_code\": county_code,\n",
    "        \"address\": site_info.get('address'),\n",
    "        \"latitude\": site_info.get('latitude'),\n",
    "        \"longitude\": site_info.get('longitude'),\n",
    "        \"parameters\": required_cols,\n",
    "        \"units\": units,\n",
    "        \"raw_file\": raw_file,\n",
    "        \"cleaned_file\": cleaned_file,\n",
    "        \"notice\": \"Cleaned file contains only rows where ALL 5 parameters were present.\"\n",
    "    }\n",
    "    \n",
    "    meta_file = f\"data/{safe_name}_metadata.json\"\n",
    "    with open(meta_file, 'w') as f:\n",
    "        json.dump(metadata, f, indent=4)\n",
    "    print(f\"  Saved Metadata: {meta_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
